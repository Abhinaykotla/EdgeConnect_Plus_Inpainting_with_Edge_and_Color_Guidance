\section{Problem Solution}
\label{sec:problem_solution}

\textbf{EdgeConnect+} introduces a novel three-stage image inpainting framework designed to address limitations in structure-only generative approaches by jointly modeling both edge and color guidance. While the original EdgeConnect framework emphasizes structural priors through a two-stage pipeline, it lacks a dedicated mechanism for ensuring chromatic consistency, often leading to desaturated or visually discordant results. To overcome this, EdgeConnect+ augments the structural pipeline with an explicit color guidance stage, creating a more semantically coherent and visually realistic output.

Our full pipeline comprises: 
(1) an edge generation network (\(G_1\)) that reconstructs structural contours from incomplete images, 
(2) a color guidance module that provides low-frequency chromatic context by blending TELEA-inpainted color maps with predicted edge maps, and 
(3) a guided image completion network (\(G_2\)) that synthesizes the final output conditioned on both structure and color priors.

Each component is carefully designed and integrated to improve both pixel-level accuracy and perceptual realism while remaining computationally tractable. The architecture is modular and extensible, making it adaptable to additional cues such as depth or semantic segmentation in future work.

\subsection{Dataset}

We evaluate our method on the CelebA dataset~\cite{liu2015deep}, a large-scale face dataset containing over 200,000 celebrity images with diverse facial attributes such as pose, expression, age, and lighting conditions. All images are center-cropped and resized to $256 \times 256$ pixels. We partition the CelebA dataset into a training set of 162,079 images, validation set of 10,129 images, and testing set of 30,391 images. Figures~\ref{fig:train_gt} and \ref{fig:test_gt} show samples from CelebA dataset.
  
\begin{figure}[h!]
    \centering
    \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/train_gt.jpg}
        \caption{}
        \label{fig:train_gt} 
    \end{minipage}\hfill
    \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/test_gt.jpg}
        \caption{}
        \label{fig:test_gt}
    \end{minipage}
\end{figure}

To simulate realistic inpainting scenarios, we apply irregular binary masks~\cite{masks} to the images, ensuring each mask covers at least 20\% of the image area. These wide masks are designed to challenge the model with substantial missing regions, including facial features and background structures. Masked regions are filled with white pixels to create input images for training. 


The preprocessing involves deriving binary masks to indicate missing regions from white areas in the images. Input edges are generated by applying the Canny edge detector to the masked image, subsequently removing edges corresponding to mask regions, thus retaining only edges from visible image areas. Grayscale versions of masked images are created as additional input for the edge generation network (G1). Ground truth edges are generated by applying the Canny edge detector to the original (unmasked) images, supervising G1 during training.


Figures~\ref{fig:gt_image} and \ref{fig:input_image} show samples from the prepared dataset.

\begin{figure}[h!]
    \centering
    \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/gt.jpg}
        \caption{Ground Truth Image}
        \label{fig:gt_image}
    \end{minipage}\hfill
    \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/input.jpg}
        \caption{Input Image}
        \label{fig:input_image}
    \end{minipage}
\end{figure}



\subsection{Methodology}
\label{sec:methodology}

\begin{figure*}[t]
  \centering
  \resizebox{\textwidth}{!}{
    \input{arch.tex}
  }
  \caption{EdgeConnect+ Inpainting Architecture.}
  \label{fig:pipeline}
\end{figure*}



\subsubsection{Edge Guidance (G\textsubscript{1})}

The first stage of the EdgeConnect+ pipeline focuses on predicting structural contours in masked regions using a dedicated edge generation network, G\textsubscript{1}. This network builds upon the encoder-decoder architecture introduced in EdgeConnect, incorporating dilated convolutions and residual connections to effectively capture both fine-grained details and global context. Unlike EdgeConnect, which requires a manually provided binary mask alongside a grayscale image as input, EdgeConnect+ extracts the binary mask directly from the masked RGB image by identifying uniformly white pixels. This automated approach eliminates the need for separate mask annotations, thereby simplifying preprocessing and improving the modularity of the pipeline.

The input to G\textsubscript{1} comprises three channels: the grayscale version of the masked image, an edge map computed using the Canny edge detector, and the automatically extracted binary mask. These components are concatenated and passed through a series of downsampling layers, multiple dilated residual blocks, and upsampling layers to generate the predicted edge map. The objective is to ensure that the predicted edges are semantically aligned with the visible structures in the unmasked regions.

The network is trained using a combination of three losses. An L\textsubscript{1} loss encourages pixel-wise accuracy, an adversarial loss facilitated by a PatchGAN-based discriminator (D\textsubscript{1}) promotes the generation of realistic edge structures, and a feature matching loss ensures perceptual stability by minimizing discrepancies in internal feature representations between real and generated outputs. While these losses are inspired by the original EdgeConnect framework, they are tailored in EdgeConnect+ to accommodate the automatically derived input masks.


\subsubsection{Color Guidance}

Following edge prediction, the second stage of the pipeline focuses on constructing a color guidance map to support the subsequent inpainting process. While EdgeConnect relies exclusively on structural priors, our method introduces explicit chromatic context to guide G\textsubscript{2} more effectively. The primary objective of this stage is to provide low-frequency color cues that promote smooth transitions and maintain chromatic coherence across masked and unmasked regions.

To construct the color guidance, we initially explored the use of Gaussian blur applied to the unmasked regions as a way to approximate the global color distribution. While Gaussian blurring effectively removes high-frequency noise, it lacks spatial awareness and often results in unnatural color transitions near mask boundaries. As a more semantically meaningful alternative, we adopt the TELEA inpainting algorithm, an efficient, non-learning-based method that fills missing regions by propagating nearby pixel values based on geometric and photometric continuity. Compared to Gaussian blur, TELEA produces smoother and more spatially coherent color priors that better preserve local structures. To further reduce edge artifacts, we apply a slight dilation to the binary mask before inpainting, softening the boundaries and minimizing residual white spaces.

After TELEA-based inpainting is applied to generate a low-frequency color prior, we refine the guidance image by overlaying the predicted edge map from G\textsubscript{1} onto the color map using a thin black stroke. This fusion process yields a composite guidance map that simultaneously encodes structural details and chromatic information. The resulting image serves as a critical input to the final inpainting generator, enhancing its ability to synthesize perceptually realistic and contextually consistent outputs.


\subsubsection{Final Inpainting (G\textsubscript{2})}

The final stage in our pipeline is handled by G2, which generates the completed image by taking in three inputs: the masked RGB image, the fused guidance map, and the binary mask. These are combined into a single 7-channel input, allowing the network to process structural and color cues together in a more unified and efficient way.

In contrast to the original EdgeConnect architecture, which employs a U-Net-based generator with skip connections and handles edge and mask inputs separately, our design adopts a more streamlined approach. G\textsubscript{2} eliminates skip connections between the encoder and the decoder, thereby reducing memory usage and architectural complexity. The network consists of initial convolutional layers for downsampling, followed by a series of residual blocks for semantic feature learning, and transposed convolutions for upsampling and image reconstruction. This simplification facilitates faster convergence and improved training stability without heavily compromising output quality.

During training, G\textsubscript{2} is supervised using a combination of loss functions. An L1 loss is applied within the masked regions to prioritize learning in areas of missing content. Perceptual and style losses, computed from feature activations of a pretrained VGG network, help preserve high-level semantics and textural consistency. In addition, an adversarial loss, facilitated by a patch based discriminator, D\textsubscript{2}, encourages the generation of visually sharp and realistic results. To maintain consistency and reproducibility during training, all input data is normalized and resized, and any missing guidance images are automatically regenerated as needed.

Figure~\ref{fig:pipeline} illustrates the overall EdgeConnect+ pipeline, highlighting the interaction between the edge generation, color guidance, and final reconstruction stages.

% \begin{figure}[h]
%   \centering
%   \input{arch.tex}
%   \caption{EdgeConnect+ Inpainting Architecture}
% \end{figure}

\subsection{Training Setup}
\label{sec:training}

EdgeConnect+ is trained on the CelebA dataset, with each training batch comprising 192 samples striking a balance between memory efficiency and convergence stability. The edge generation network (G\textsubscript{1}) is trained for 25 epochs, followed by 5 epochs of training for the image completion network (G\textsubscript{2}). Both networks are optimized using the Adam optimizer, configured with a learning rate of $1 \times 10^{-4}$ and a weight decay of $5 \times 10^{-5}$ to encourage stable convergence and reduce overfitting.

To accommodate fused edge and color guidance while maintaining computational efficiency, EdgeConnect+ is designed with a leaner architecture compared to its predecessor. Whereas the original EdgeConnect framework utilizes approximately 22 million parameters~\cite{hertz2023any} across its generators, EdgeConnect+ operates with a reduced footprint of roughly 21.5 million parameters, distributed across G\textsubscript{1} and G\textsubscript{2}. Despite the lighter architecture, the model is capable of processing richer input representations by incorporating structural and chromatic cues, leading to perceptually and structurally coherent inpainting results.

Training is performed using mixed-precision arithmetic with gradient scaling, which accelerates computation while preserving numerical stability. An Exponential Moving Average (EMA) is maintained over the generator weights to smooth updates and improve generalization. To mitigate overfitting, an early stopping mechanism halts training if validation loss fails to improve for five consecutive epochs.

The training loops are modular, fault-tolerant, and fully instrumented. All training progress is logged, including loss trends and key metrics, with periodic checkpointing that enables seamless interruption and resumption. Scripts are integrated to save generated sample outputs every 200 batches (configurable), allowing for real-time qualitative monitoring. Additionally, the training setup supports on-the-fly modification of loss weights based on observed outputs, enabling dynamic tuning of hyperparameters mid-training. This design facilitates iterative experimentation, making it possible to resume from a previous checkpoint while adjusting model behavior to improve convergence or visual fidelity.

All experiments are conducted on CUDA-enabled NVIDIA A100 GPUs, leveraging GPU parallelism for efficient training of large-scale generative models.



\subsection{Loss Functions}

The EdgeConnect+ framework consists of two generator–discriminator pairs: the edge generation network \(G_1\) and its discriminator \(D_1\), followed by the inpainting generator \(G_2\) and its corresponding discriminator \(D_2\). Each component is trained using a composite of loss functions designed to encourage structural accuracy, perceptual fidelity, and stylistic realism.

\textbf{Pixel Reconstruction Loss (L1):}  
Both \(G_1\) and \(G_2\) are trained with an L1 pixel reconstruction loss to ensure that the outputs, edge maps in the case of \(G_1\), and completed images for \(G_2\) remain close to the ground truth at a pixel level:
\[
\mathcal{L}_{\text{L1}} = \| y - \hat{y} \|_1
\]
where \(y\) and \(\hat{y}\) represent the ground truth and the predicted outputs, respectively.

\textbf{Adversarial Loss:}  
To improve realism, both generators are trained adversarially using PatchGAN based discriminators (\(D_1\), \(D_2\)), which evaluate the authenticity of local image patches:
\[
\mathcal{L}_{\text{adv}} = \mathbb{E}_{x, y}[\log D(x, y)] + \mathbb{E}_{x, \hat{y}}[\log(1 - D(x, \hat{y}))]
\]

\textbf{Gradient Penalty:}  
To stabilize discriminator training and enforce a soft Lipschitz constraint, we incorporate a gradient penalty term:
\[
\mathcal{L}_{\text{GP}} = \mathbb{E}_{\hat{x}} \left[ (\| \nabla_{\hat{x}} D(\hat{x}) \|_2 - 1)^2 \right]
\]
where \(\hat{x}\) is an interpolated sample between real and generated data.

\textbf{Feature Matching Loss:}  
To reduce mode collapse and stabilize adversarial training, a feature matching loss is used:
\[
\mathcal{L}_{\text{FM}} = \sum_{i=1}^{L} \| D_i(x, y) - D_i(x, \hat{y}) \|_1
\]
which compares internal feature activations of the discriminator between real and generated outputs.

\textbf{Perceptual and Style Losses (G\textsubscript{2} only):}  
For the image synthesis network \(G_2\), we add perceptual and style-based losses using a pre-trained VGG16 network. The perceptual loss is defined as:
\[
\mathcal{L}_{\text{perc}} = \sum_{l} \| \phi_l(I) - \phi_l(\hat{I}) \|_1
\]
and the style loss, derived from Gram matrices of VGG features, as:
\[
\mathcal{L}_{\text{style}} = \sum_l \| G_l(I) - G_l(\hat{I}) \|_1
\]

\textbf{Total Loss Formulations:}  
The full objective functions for each generator and discriminator are:

\[
\mathcal{L}_{G_1} = \lambda_{e1} \mathcal{L}_{\text{L1}} + \lambda_{e2} \mathcal{L}_{\text{adv}} + \lambda_{e3} \mathcal{L}_{\text{FM}}
\]
\[
\mathcal{L}_{D_1} = -\mathcal{L}_{\text{adv}} + \lambda_{gp} \mathcal{L}_{\text{GP}}
\]
\[
\mathcal{L}_{G_2} = \lambda_1 \mathcal{L}_{\text{L1}} + \lambda_2 \mathcal{L}_{\text{adv}} + \lambda_3 \mathcal{L}_{\text{perc}} + \lambda_4 \mathcal{L}_{\text{style}} + \lambda_5 \mathcal{L}_{\text{FM}}
\]
\[
\mathcal{L}_{D_2} = -\mathcal{L}_{\text{adv}} + \lambda_{gp} \mathcal{L}_{\text{GP}}
\]

Here, \(\lambda_{e1}\) through \(\lambda_{e3}\), and \(\lambda_1\) through \(\lambda_5\), are empirically chosen weights for balancing the individual loss components. The gradient penalty weight \(\lambda_{gp}\) plays a critical role in ensuring discriminator stability. In our experiments, these losses collectively contribute to producing inpainted outputs that balance pixel-level accuracy with high perceptual and structural quality.

Figures~\ref{fig:loss_trends} and \ref{fig:g2_loss_trends} visualize the training progression of each loss term.


\subsection{Evaluation Metrics}
\label{sec:evaluation}

We evaluate EdgeConnect+ using standard metrics that assess both pixel-level fidelity and perceptual quality. Although the model is not fully trained, preliminary results show promising improvements across several dimensions, suggesting the potential of combining structural and chromatic guidance.

\textbf{PSNR:}  
Peak Signal-to-Noise Ratio evaluates reconstruction fidelity by comparing pixel-level differences. EdgeConnect+ achieves a PSNR of 25.23, slightly lower than EdgeConnect’s 25.28. This marginal difference may be attributed to the model’s emphasis on perceptual realism rather than strict pixel-level matching.

\textbf{SSIM:}  
The Structural Similarity Index (SSIM) measures perceptual quality in terms of luminance, contrast, and structure. EdgeConnect+ achieves 0.864 compared to 0.846 for EdgeConnect, suggesting improved semantic coherence due to integrated edge and color guidance.

\textbf{$\ell_1$ Loss:}  
EdgeConnect+ reports a slightly higher $\ell_1$ error (4.83\%) versus EdgeConnect (3.03\%), which aligns with the design goal of prioritizing perceptual alignment over exact pixel recovery.

\textbf{FID:}  
Fréchet Inception Distance (FID) evaluates image realism and diversity. EdgeConnect+ achieves a FID score of 2.94, indicating reasonably good alignment with natural image distributions, though slightly less effective than the original EdgeConnect (FID 2.82), potentially due to limited training or added model complexity.

\textbf{LPIPS:}  
The LPIPS metric measures perceptual similarity using deep feature comparisons. With a score of 0.193, EdgeConnect+ shows encouraging perceptual closeness to the ground truth. Since LPIPS was not reported for EdgeConnect, this serves as a supplemental indication of perceptual improvements.

These initial results suggest that integrating edge and color priors can positively influence inpainting performance. We anticipate that further training and tuning will enhance these metrics further and strengthen the model's performance relative to established baselines.


\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Metric} & \textbf{Fusion Label~\cite{shao2021generative}} & \textbf{EdgeConnect} & \textbf{Ours} \\
\hline
PSNR & 29.16 & 25.28 & 25.23 \\  
SSIM & 0.9235 & 0.846 & 0.864 \\
$\ell_1$ Loss (\%) & Not reported & 3.03 & 4.83 \\
FID & Not reported & 2.82 & 2.94 \\
LPIPS & Not reported & Not reported & 0.193 \\
\hline
\end{tabular}
\caption{Quantitative comparison of inpainting performance on the CelebA dataset of different models: Fusion label, EdgeConnect and EdgeConnect+}
\label{tab:quant_comparison}
\end{table}


Table~\ref{tab:quant_comparison} presents quantitative results comparing the performance of EdgeConnect, our proposed EdgeConnect+, and the fusion-based method introduced by the paper - \textit{Generative image inpainting via edge structure and color aware fusion} ~\cite{shao2021generative}. We include the Fusion Label model in this comparison as it also combines edge and color information using a dual-encoder architecture with gated feature fusion and spatial-channel attention to guide the final inpainting. Including this baseline offers a meaningful point of comparison, helping contextualize the effectiveness of our own modular guidance strategy in balancing structural alignment and perceptual quality.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{images/metric_comparison.png}
    \caption{Visual comparison of evaluation metrics between EdgeConnect and EdgeConnect+ on CelebA.}
    \label{fig:metric_comparison}
\end{figure}
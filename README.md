
# EdgeConnect+: Adversarial Inpainting with Edge and Color Guidance

EdgeConnect+ is a deep learning-based image inpainting framework that enhances the original [EdgeConnect](https://arxiv.org/abs/1901.00212) model. This project integrates both **edge structure** and **color guidance** to produce perceptually realistic reconstructions of masked images.

ğŸ“„ [Final Paper (PDF)](EdgeConnectplus_Adversarial_Inpainting_with_Edge_and_Color_Guidance.pdf) 

---

## ğŸ§  Overview

The model is divided into a three-stage pipeline:

1. **Edge Generation (G1)**: Predicts edges in masked regions using grayscale input and binary masks. 
2. **Color Guidance**: Provides low-frequency chromatic information from the unmasked image regions and blends it with predicted edges to guide final image inpainting.
3. **Final Inpainting (G2)**: Synthesizes the completed image using combined structure and color guidance.

---

## ğŸ§± Model Architecture

Below is the complete architecture diagram showing the flow from input to final output:

![EdgeConnect+ Architecture](Docs/Images/Edgeconnectplus_architecture.png)

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ config.py                  # Global configuration
â”œâ”€â”€ train.py                   # Main entry point
â”œâ”€â”€ dataloader_g1.py           # Dataloader for G1
â”œâ”€â”€ dataloader_g2.py           # Dataloader for G2
â”œâ”€â”€ g1_model.py                # Generator and Discriminator for G1
â”œâ”€â”€ g2_model.py                # Generator and Discriminator for G2
â”œâ”€â”€ loss_functions.py          # All loss definitions (L1, Adv, FM, Perceptual, Style)
â”œâ”€â”€ train_loops_g1.py          # Training loop for G1
â”œâ”€â”€ train_loops_g2.py          # Training loop for G2
â”œâ”€â”€ utils_dl.py                # Dataset utilities
â”œâ”€â”€ utils_g1.py                # Utilities for training G1 (saving, evaluation, etc.)
â”œâ”€â”€ utils_g2.py                # Utilities for training G2
â”œâ”€â”€ find_lr.py                 # Learning rate finder
â”œâ”€â”€ demo.py                    # Inference script for testing / demo
â”œâ”€â”€ evaluate_models.py         # Computes evaluation metrics (PSNR, SSIM, L1, LPIPS, FID)
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # This file
```

---

## ğŸ§ª Datasets

- **CelebA Dataset**: All images are center-cropped and resized to 256Ã—256.
- Irregular binary masks generated with â‰¥ 20% coverage.
- Masks applied during preprocessing to create white-hole inputs.
- Ground truth edges from Canny on original images.
- Input edges computed from masked image with mask-edges removed.

---

## âš™ï¸ Training Setup

- Batch Size: 192  
- Epochs: 25 (G1), 5 (G2)
- Optimizer: Adam (lr = 1e-4, weight_decay = 5e-5, Î²â‚ = 0.0, Î²â‚‚ = 0.9)  
- Architecture: ~21.5M parameters across G1 and G2  
- Precision: Mixed Precision (AMP) with Gradient Scaling  
- Stabilization: Exponential Moving Average (EMA) on generator weights  
- Early Stopping: Stops after 5 epochs without validation improvement  
- Checkpointing: Periodic, resume-safe; sample outputs saved every 200 batches  
- Tuning: Supports live loss-weight adjustment during training  
- Hardware: Trained on CUDA-enabled NVIDIA A100 GPUs  

---

## ğŸš€ Getting Started

Follow the steps below to set up and run the EdgeConnect+ inpainting pipeline:


### 1ï¸âƒ£ Clone the Repository

```
git clone https://github.com/Abhinaykotla/EdgeConnect_Plus_Inpainting_with_Edge_and_Color_Guidance.git

cd EdgeConnect_Plus_Inpainting_with_Edge_and_Color_Guidance
```

### 2ï¸âƒ£ Install Dependencies

We recommend using Python â‰¥ 3.8 and a virtual environment.

```
pip install -r requirements.txt
```

### 3ï¸âƒ£ Prepare the Data

Organize your image data and masks into the following folder structure:

```
data_archive/
â””â”€â”€ CelebA/
  â”œâ”€â”€ train_input/ # masked images
  â”œâ”€â”€ train_gt/ # original ground truth images
  â”œâ”€â”€ val_input/
  â”œâ”€â”€ val_gt/
  â”œâ”€â”€ test_input/
  â””â”€â”€ test_gt/
```
Input images can be created using random masks with the help of notebooks\celeba_data_prep.ipynb

Ensure that:
- All images are 256Ã—256 `.jpg` files
- Masked images use white (255,255,255) pixels for missing regions


### 4ï¸âƒ£ Start Training (G1 + G2)

```
python train.py
```

- Trains the full pipeline:  
  - G1 (Edge Generator) + D1 (Discriminator)  
  - G2 (Inpainting Generator) + D2 (Discriminator)  
- Inputs:
  - For G1: grayscale image, canny edge map, binary mask  
  - For G2: masked RGB image, color guidance (edge + color), binary mask  
- Outputs:
  - From G1: Edge maps
  - From G2: Final inpainted RGB images
  - Checkpoints, loss curves, sample visualizations in the `models/` folder

To resume training from checkpoints, simply re-run the same command.

### 5ï¸âƒ£ Run Inference / Testing

To test the trained model on new inputs:

```
python demo.py
```

Make sure your test input folder is properly set in `config.py`. The script will:
- Load the latest checkpoints
- Generate color guidance
- Save final inpainted results to `output/` or `models/generated_samples_g2/`

### 6ï¸âƒ£ Evaluate Results (Optional)

To compute PSNR, SSIM, L1, LPIPS and FID:

```
python evaluate_models.py
```

You can customize evaluation parameters and paths in `config.py`.
 
---

## ğŸ§ª Evaluation Metrics

| Metric       | EdgeConnect | EdgeConnect+ (Ours) |
|--------------|-------------|---------------------|
| **PSNR**     | 25.28       | 25.23               |
| **SSIM**     | 0.846       | 0.864               |
| **L1 Loss**  | 3.03%       | 4.83%               |
| **FID**      | 2.82        | 2.94                |
| **LPIPS**    | â€”           | 0.193               |

While PSNR and L1 are slightly lower due to perceptual prioritization, EdgeConnect+ demonstrates **stronger structure preservation and realism** in challenging regions.

---

## ğŸ“· Sample Output

![EdgeConnect+ Architecture](Docs/Images/SampleOutput_2.png)
  - Top: Masked input, color guidance, final output  
  - Bottom: Binary mask, ground truth, absolute error map

The difference map, which is largely dark, indicates high alignment between prediction and ground truth,
demonstrating the potential of the model for high-quality
image reconstruction.

---

## ğŸ“ Loss Formulations

### Generator Losses

**Edge Generator (G1)** uses:
L_G1 = Î»â‚ * L1 + Î»â‚‚ * Adversarial + Î»â‚ƒ * FeatureMatching

**Inpainting Generator (G2)** uses:
L_G2 = Î»â‚ * L1 + Î»â‚‚ * Adversarial + Î»â‚ƒ * Perceptual + Î»â‚„ * Style + Î»â‚… * FeatureMatching

Each term captures a different property:  
- **L1**: pixel-wise accuracy  
- **Adversarial**: realism through PatchGAN  
- **Perceptual, Style**: high-level consistency via pretrained VGG  
- **Feature Matching**: stabilizes GAN training

### Discriminator Losses

To enforce discriminator regularization and improve GAN stability, **Gradient Penalty (GP)** is added to both D1 and D2.

**Final Discriminator Loss**:
L_D = âˆ’L_adv + Î»_gp * L_GP

---

## ğŸ”­ Future Work

EdgeConnect+ lays the groundwork for a powerful dual-guided inpainting framework, and several promising directions remain open for enhancement:

- **Extended Training and Scaling**: With additional computational resources, longer training of the inpainting stage (G2) can unlock finer textures, stronger semantic alignment, and improved generalization.
- **Semantic Conditioning**: Incorporating high-level priors (e.g., segmentation, vision-language models) may enable controllable inpainting.
- **Learnable Fusion**: Replacing fixed guidance overlays with attention-based or adaptive fusion could enhance flexibility.
- **Dataset Generalization**: The modular design can extend beyond facial data to natural scenes, bodies, and text-rich contexts.
- **End-to-End Color Guidance**: Learning color priors instead of handcrafting them may yield more expressive, trainable inputs.

---

## ğŸ¤ Contributors

- **Abhinay Kotla** â€” [@abhinaykotla](https://github.com/Abhinaykotla)  
- **Sanjana Ravi Prakash** â€” [@sanjanarp](https://github.com/sanjanarp)

---

## ğŸ§  Citation

```
@misc{edgeconnectplus2025,
  author = {Abhinay Kotla and Sanjana Ravi Prakash},
  title = {EdgeConnect+: Adversarial Inpainting with Edge and Color Guidance},
  year = {2025},
  note = {University of Texas at Arlington, CSE 6367}
}
```

---

## ğŸ“„ License

This repository is shared for educational and research purposes.

---

## ğŸ“¬ Acknowledgements

- [EdgeConnect (Nazeri et al.)](https://arxiv.org/abs/1901.00212)
- [CelebA Dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)

---
